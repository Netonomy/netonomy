version: "3.0"
services:
  # llama-cpp:
  #   image: ghcr.io/abetlen/llama-cpp-python:latest
  #   ports:
  #     - 8000:8000
  #   volumes:
  #     - ./models:/models
  #   environment:
  #     - MODEL=/models/llama/llama-2-7b-chat.Q4_K_M.gguf
  #     - LLAMA_METAL=on
  #     - -DLLAMA_METAL=on
  #     - n_gpu_layers=1

  frontend:
    build: ./web-frontend
    ports:
      - 4000:4000
    volumes:
      - ./web-frontend:/app
    depends_on:
      - server

  server:
    build: ./server
    ports:
      - 3000:3000
    volumes:
      - ./server:/usr/src/app
    depends_on:
      - vector-store

  vector-store:
    image: chromadb/chroma
    volumes:
      - ./chroma-data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
    ports:
      - 8000:8000

  postgres:
    image: postgres
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres
    volumes:
      - ./.postgresql/data:/var/lib/postgresql/data
      - ./db.sql:/docker-entrypoint-initdb.d/db.sql
    ports:
      - 5432:5432

  dwn-server:
    image: ghcr.io/tbd54566975/dwn-server:main
    ports:
      - 3100:3100
    volumes:
      - ./.dwn-data/:/dwn-server/data
    environment:
      - DS_PORT=3100
      - DS_MAX_RECORD_DATA_SIZE=100gb
    platform: linux/amd64
