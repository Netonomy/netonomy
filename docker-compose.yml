version: "3.0"
services:
  # llama-cpp:
  #   image: ghcr.io/abetlen/llama-cpp-python:latest
  #   ports:
  #     - 8000:8000
  #   volumes:
  #     - ./models:/models
  #   environment:
  #     - MODEL=/models/llama/llama-2-7b-chat.Q4_K_M.gguf
  #     - LLAMA_METAL=on
  #     - -DLLAMA_METAL=on
  #     - n_gpu_layers=1

  frontend:
    build: ./web-frontend
    ports:
      - 4000:4000
    volumes:
      - ./web-frontend:/app
    depends_on:
      - server 

  server:
    build: ./server
    ports:
      - 3000:3000
    volumes:
      - ./server:/usr/src/app
    depends_on:
      - vector-store

  vector-store:
    image: chromadb/chroma
    volumes:
      - ./chroma-data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
    ports:
      - 8000:8000
